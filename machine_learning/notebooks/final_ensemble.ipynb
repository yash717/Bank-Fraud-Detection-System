{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "file_path=\"../data/transaction_detail.csv\"\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(\"../data/transaction_detail.csv\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def train_models(X_train, y_train):\n",
    "    # XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Random Forest model\n",
    "    rf_model = RandomForestClassifier()\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Neural Network model\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    model_nn = keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(8, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model_nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model_nn.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "    return xgb_model, rf_model, model_nn\n",
    "\n",
    "def train_ensemble(xgb_model, rf_model, model_nn, X_train, y_train):\n",
    "    # Ensemble model\n",
    "    ensemble_model = VotingClassifier(estimators=[\n",
    "        ('xgb', xgb_model),\n",
    "        ('rf', rf_model),\n",
    "        ('nn', KerasClassifier(build_fn=model_nn, epochs=1, batch_size=32, verbose=0))\n",
    "    ], voting='soft')  # Use 'soft' for averaging probabilities, 'hard' for voting\n",
    "\n",
    "    # Fit the ensemble model\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "    return ensemble_model\n",
    "\n",
    "def save_model(model, file_name):\n",
    "    with open(file_name, 'wb') as model_file:\n",
    "        pkl.dump(model, model_file)\n",
    "\n",
    "\n",
    "def preprocess_input(input_data):\n",
    "    # Create a DataFrame from the input data\n",
    "    processed_input = pd.DataFrame(input_data)\n",
    "\n",
    "    # Convert 'Transaction_Date' to datetime\n",
    "    processed_input['Transaction_Date'] = pd.to_datetime(processed_input['Transaction_Date'], format='%Y-%m-%d')\n",
    "\n",
    "    # Extract additional features from datetime columns if needed\n",
    "    processed_input['Transaction_Year'] = processed_input['Transaction_Date'].dt.year\n",
    "    processed_input['Transaction_Month'] = processed_input['Transaction_Date'].dt.month\n",
    "    processed_input['Transaction_Day'] = processed_input['Transaction_Date'].dt.day\n",
    "    processed_input['Transaction_Hour'] = processed_input['Transaction_Date'].dt.hour\n",
    "    processed_input['Transaction_Minute'] = processed_input['Transaction_Date'].dt.minute\n",
    "\n",
    "    # Drop original datetime columns\n",
    "    processed_input = processed_input.drop(['Transaction_Date'], axis=1)\n",
    "\n",
    "    # Perform one-hot encoding for categorical columns\n",
    "    processed_input = pd.get_dummies(processed_input)\n",
    "\n",
    "    return processed_input\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "    return accuracy, conf_matrix, class_report\n",
    "\n",
    "# Load and preprocess data\n",
    "file_path = '../data/transaction_detail.csv'\n",
    "df = load_data(file_path)\n",
    "data = preprocess_input(df)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data.drop('Fraud_Label', axis=1)\n",
    "y = df['Fraud_Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 248)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fraud(ensemble_model, input_data):\n",
    "    # Load the ensemble model from the pickle file\n",
    "    with open(ensemble_model, 'rb') as model_file:\n",
    "        loaded_model = pkl.load(model_file)\n",
    "\n",
    "    # Preprocess the input data\n",
    "    processed_input = input_data # preprocess_input(input_data)\n",
    "\n",
    "    # Make predictions using the loaded model\n",
    "    fraud_prediction = loaded_model.predict(processed_input)\n",
    "\n",
    "    return fraud_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1500 104 500 500 7 80 0 0 2022 11 15 0 0 True True True True True]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Provided input data\n",
    "input_data1 ={\n",
    "    'Transaction_Amount': [1500],\n",
    "    'User_Account_ID': [104],\n",
    "    'Transaction_Date': ['2022-11-15'],\n",
    "    'Payment_Method': ['Credit Card'],\n",
    "    'Billing_Location': ['Bangalore'],\n",
    "    'Shipping_Location': ['Hyderabad'],\n",
    "    'Device_IP_Address': ['192.168.1.40'],\n",
    "    'Session_Duration': [500],\n",
    "    'Transaction_Time': [500],\n",
    "    'Frequency_of_Transactions': [7],\n",
    "    'Time_Between_Transactions': [80],\n",
    "    'Unusual_Time_of_Transaction': [0],\n",
    "    'Unusual_Transaction_Amounts': [0],\n",
    "    'IP_Address_History': ['192.168.1.40'],\n",
    "}\n",
    "\n",
    "a=preprocess_input(input_data1)\n",
    "\n",
    "# Create a DataFrame from the input data\n",
    "input_df = pd.DataFrame(a)\n",
    "\n",
    "# Convert the DataFrame to a NumPy array\n",
    "input_array = input_df.to_numpy()\n",
    "\n",
    "# Display the NumPy array\n",
    "print(input_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Payment_Method', 'Other_Categorical_Columns'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Yash\\Desktop\\fraud_detection_project\\machine_learning\\notebooks\\final_ensemble.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Yash/Desktop/fraud_detection_project/machine_learning/notebooks/final_ensemble.ipynb#X12sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m categorical_columns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mPayment_Method\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mOther_Categorical_Columns\u001b[39m\u001b[39m'\u001b[39m]  \u001b[39m# Replace with actual categorical columns\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Yash/Desktop/fraud_detection_project/machine_learning/notebooks/final_ensemble.ipynb#X12sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m \u001b[39m# Perform one-hot encoding for categorical columns for training data\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Yash/Desktop/fraud_detection_project/machine_learning/notebooks/final_ensemble.ipynb#X12sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m X_train_encoded \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mget_dummies(X_train, columns\u001b[39m=\u001b[39;49mcategorical_columns)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Yash/Desktop/fraud_detection_project/machine_learning/notebooks/final_ensemble.ipynb#X12sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m \u001b[39m# Create a mapping of columns from the training dataset after encoding\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Yash/Desktop/fraud_detection_project/machine_learning/notebooks/final_ensemble.ipynb#X12sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m encoded_columns \u001b[39m=\u001b[39m X_train_encoded\u001b[39m.\u001b[39mcolumns\n",
      "File \u001b[1;32mc:\\Users\\Yash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\encoding.py:158\u001b[0m, in \u001b[0;36mget_dummies\u001b[1;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInput must be a list-like for parameter `columns`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    157\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     data_to_encode \u001b[39m=\u001b[39m data[columns]\n\u001b[0;32m    160\u001b[0m \u001b[39m# validate prefixes and separator to avoid silently dropping cols\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_len\u001b[39m(item, name):\n",
      "File \u001b[1;32mc:\\Users\\Yash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3765\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3766\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 3767\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   3769\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3770\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Yash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5876\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5873\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   5874\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5876\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   5878\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   5879\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5880\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5935\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5933\u001b[0m     \u001b[39mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   5934\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 5935\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   5937\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[0;32m   5938\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['Payment_Method', 'Other_Categorical_Columns'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import pickle as pkl\n",
    "\n",
    "# Load data from a CSV file\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(\"../data/transaction_detail.csv\")\n",
    "    return df\n",
    "\n",
    "# Preprocess the input data\n",
    "def preprocess_input(input_data):\n",
    "    # Drop unnecessary columns\n",
    "    cols_to_drop = ['Transaction_ID', 'Transaction_Time', 'User_Account_ID', 'Account_Creation_Date', 'IP_Address_History']\n",
    "    processed_input = input_data.drop(cols_to_drop, axis=1)\n",
    "\n",
    "    # Convert 'Transaction_Date' to datetime\n",
    "    processed_input['Transaction_Date'] = pd.to_datetime(processed_input['Transaction_Date'], format='%Y-%m-%d')\n",
    "\n",
    "    # Perform one-hot encoding for categorical columns\n",
    "    processed_input = pd.get_dummies(processed_input, columns=['Payment_Method', 'Billing_Location', 'Shipping_Location'])\n",
    "\n",
    "    return processed_input\n",
    "\n",
    "# Train models using XGBoost, Random Forest, and Neural Network\n",
    "def train_models(X_train, y_train):\n",
    "    # XGBoost model\n",
    "    xgb_model = XGBClassifier()\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Random Forest model\n",
    "    rf_model = RandomForestClassifier()\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Neural Network model\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    model_nn = keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model_nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model_nn.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "    return xgb_model, rf_model, model_nn\n",
    "\n",
    "# Train an ensemble model using the trained models\n",
    "def train_ensemble(xgb_model, rf_model, model_nn, X_train, y_train):\n",
    "    ensemble_model = VotingClassifier(estimators=[\n",
    "        ('xgb', xgb_model),\n",
    "        ('rf', rf_model),\n",
    "        ('nn', KerasClassifier(build_fn=model_nn, epochs=1, batch_size=32, verbose=0))\n",
    "    ], voting='soft')\n",
    "\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    return ensemble_model\n",
    "\n",
    "# Save the trained ensemble model to a file\n",
    "def save_model(model, file_name):\n",
    "    with open(file_name, 'wb') as model_file:\n",
    "        pkl.dump(model, model_file)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'path_to_your_file.csv'  # Replace with the actual path to your dataset file\n",
    "df = load_data(file_path)\n",
    "\n",
    "# Preprocess the data\n",
    "data = preprocess_input(df)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data.drop('Fraud_Label', axis=1)\n",
    "y = data['Fraud_Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Train models\n",
    "xgb_model, rf_model, model_nn = train_models(X_train, y_train)\n",
    "\n",
    "# Train the ensemble model\n",
    "ensemble_model = train_ensemble(xgb_model, rf_model, model_nn, X_train, y_train)\n",
    "\n",
    "# Save the ensemble model\n",
    "save_model(ensemble_model, 'ensemble_model.pkl')\n",
    "\n",
    "# Predict with new input data\n",
    "def predict_with_new_input(new_input_data):\n",
    "    new_data = preprocess_input(pd.DataFrame(new_input_data))\n",
    "    new_data_aligned = new_data.reindex(columns=X.columns, fill_value=0)\n",
    "    predictions = ensemble_model.predict(new_data_aligned)\n",
    "    return predictions\n",
    "\n",
    "# Example usage of predict_with_new_input function\n",
    "new_input_data = {\n",
    "    'Transaction_Amount': [1200],\n",
    "    'Transaction_Date': ['2023-06-01'],\n",
    "    # Add other relevant columns similar to your dataset\n",
    "}\n",
    "\n",
    "new_predictions = predict_with_new_input(new_input_data)\n",
    "print(new_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
